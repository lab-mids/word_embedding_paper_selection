{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "from matnexus import VecGenerator\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class WorkflowPipeline:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with a configuration dictionary.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.random_seed = config.get('random_seed', 42)\n",
    "        self.PERIODIC_TABLE_ELEMENTS = [\n",
    "            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P',\n",
    "            'S', 'Cl', 'Ar', 'K', 'Ca',\n",
    "            'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se',\n",
    "            'Br', 'Kr', 'Rb', 'Sr', 'Y',\n",
    "            'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I',\n",
    "            'Xe', 'Cs', 'Ba', 'La', 'Ce',\n",
    "            'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf',\n",
    "            'Ta', 'W', 'Re', 'Os', 'Ir',\n",
    "            'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa',\n",
    "            'U', 'Np', 'Pu', 'Am', 'Cm',\n",
    "            'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds',\n",
    "            'Rg', 'Cn', 'Nh', 'Fl', 'Mc',\n",
    "            'Lv', 'Ts', 'Og'\n",
    "        ]\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "    def train_doc2vec(self, abstracts_df):\n",
    "        params = self.config['doc2vec']\n",
    "\n",
    "        # Filter and create a new copy to avoid SettingWithCopyWarning\n",
    "        abstracts_df = abstracts_df[abstracts_df['abstract'].notna()].copy()\n",
    "        abstracts_df['abstract'] = abstracts_df['abstract'].astype(str)\n",
    "\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=abstract.split(), tags=[str(i)])\n",
    "            for i, abstract in enumerate(abstracts_df['abstract'])\n",
    "        ]\n",
    "        model = Doc2Vec(\n",
    "            tagged_docs,\n",
    "            vector_size=params.get('vector_size', 50),\n",
    "            window=params.get('window', 5),\n",
    "            min_count=params.get('min_count', 1),\n",
    "            workers=params.get('workers', 4),\n",
    "            seed=self.random_seed\n",
    "        )\n",
    "        self.doc2vec_model = model\n",
    "        self.paper_vectors = np.array([model.dv[str(i)] for i in range(len(tagged_docs))])\n",
    "        return model\n",
    "\n",
    "    def train_word2vec(self, abstracts_df, output_path=None):\n",
    "        params = self.config['word2vec']\n",
    "        corpus = VecGenerator.Corpus(abstracts_df)\n",
    "        sentences = corpus.sentences\n",
    "        model = VecGenerator.Word2VecModel(sentences)\n",
    "        model.fit(\n",
    "            sg=params.get('sg', 1),\n",
    "            vector_size=params.get('vector_size', 100),\n",
    "            hs=params.get('hs', 1),\n",
    "            window=params.get('window', 5),\n",
    "            min_count=params.get('min_count', 1),\n",
    "            workers=params.get('workers', 4)\n",
    "        )\n",
    "        if output_path:  # Save only if an output path is specified\n",
    "            model.save(str(output_path))  # Convert Path to string\n",
    "        self.word2vec_model = model  # Keep the model in memory\n",
    "        return model\n",
    "\n",
    "    def process_materials(self, material_df):\n",
    "        params = self.config['materials_processing']\n",
    "        property_list = params['property_list']\n",
    "        elements = [col for col in material_df.columns if col in self.PERIODIC_TABLE_ELEMENTS]\n",
    "\n",
    "        if not hasattr(self, 'word2vec_model'):\n",
    "            raise ValueError(\"Word2Vec model not found. Ensure the model is trained before processing materials.\")\n",
    "\n",
    "        self.calculator = VecGenerator.MaterialSimilarityCalculator(self.word2vec_model)\n",
    "\n",
    "        for prop in property_list:\n",
    "            similarity_col = f\"Similarity_to_{prop}\"\n",
    "            try:\n",
    "                temp_df = self.calculator.calculate_similarity_from_dataframe(\n",
    "                    material_df,\n",
    "                    elements,\n",
    "                    target_property=[prop],\n",
    "                    add_experimental_indicator=False\n",
    "                )\n",
    "                material_df[similarity_col] = temp_df['Similarity']\n",
    "\n",
    "                if 'Material_Vec' in temp_df.columns:\n",
    "                    material_df['Material_Vec'] = temp_df['Material_Vec']\n",
    "                else:\n",
    "                    material_df['Material_Vec'] = np.nan\n",
    "            except Exception as e:\n",
    "                material_df[similarity_col] = np.nan\n",
    "\n",
    "        return material_df\n",
    "    def greedy_selection(self):\n",
    "        \"\"\"\n",
    "        Optimized greedy selection:\n",
    "        - Uses vectorized operations for distance calculations.\n",
    "        - Tracks minimum distances for unselected vectors to reduce redundant computations.\n",
    "        \"\"\"\n",
    "        params = self.config['greedy_selection']\n",
    "        vectors = self.paper_vectors\n",
    "\n",
    "        if params.get('use_pca', False):\n",
    "            n_components = params.get('n_components', 2)\n",
    "            pca = PCA(n_components=n_components)\n",
    "            vectors = pca.fit_transform(vectors)\n",
    "\n",
    "        # Parameters\n",
    "        start_size = params.get('start_size', 50)\n",
    "        step_size = params.get('step_size', 50)\n",
    "        method = params.get('method', 'cosine')\n",
    "\n",
    "        # Initialize selection for the first step\n",
    "        if not hasattr(self, 'selected_indices'):\n",
    "            # First selection: randomly pick the initial vector\n",
    "            np.random.seed(self.random_seed)\n",
    "            first_index = np.random.choice(len(vectors))\n",
    "            self.selected_indices = [first_index]\n",
    "            self.remaining_indices = list(range(len(vectors)))\n",
    "            self.remaining_indices.remove(first_index)\n",
    "\n",
    "            # Calculate initial distances from all remaining vectors to the first selected vector\n",
    "            distances = self._compute_distances(vectors, first_index, method)\n",
    "            self.min_distances = distances.copy()  # Track minimum distances for efficiency\n",
    "\n",
    "            # Select the rest of the first batch\n",
    "            while len(self.selected_indices) < start_size:\n",
    "                self._select_next_furthest(vectors, method)\n",
    "\n",
    "            # Set the target size for subsequent steps\n",
    "            self.target_size = start_size\n",
    "\n",
    "        else:\n",
    "            # Increment target size for this batch\n",
    "            self.target_size += step_size\n",
    "\n",
    "            # Add only the new `step_size` papers\n",
    "            while len(self.selected_indices) < self.target_size:\n",
    "                self._select_next_furthest(vectors, method)\n",
    "\n",
    "        return self.selected_indices\n",
    "\n",
    "    def _select_next_furthest(self, vectors, method):\n",
    "        \"\"\"\n",
    "        Helper method to select the next furthest vector using tracked minimum distances.\n",
    "        \"\"\"\n",
    "        # Identify the furthest vector from the selected set\n",
    "        furthest_index = np.argmax(self.min_distances)\n",
    "        actual_index = self.remaining_indices[furthest_index]\n",
    "\n",
    "        # Append the furthest vector to the selected set\n",
    "        self.selected_indices.append(actual_index)\n",
    "\n",
    "        # Remove the selected vector from the remaining set\n",
    "        del self.remaining_indices[furthest_index]\n",
    "\n",
    "        # Remove the corresponding entry in min_distances\n",
    "        self.min_distances = np.delete(self.min_distances, furthest_index)\n",
    "\n",
    "        # Compute new distances for the remaining vectors\n",
    "        new_distances = self._compute_distances(vectors, actual_index, method)\n",
    "\n",
    "        # Update the minimum distances\n",
    "        self.min_distances = np.minimum(self.min_distances, new_distances)\n",
    "\n",
    "    def _compute_distances(self, vectors, selected_index, method):\n",
    "        \"\"\"\n",
    "        Compute distances from one vector to all remaining vectors using the specified method.\n",
    "        \"\"\"\n",
    "        remaining_vectors = vectors[self.remaining_indices]\n",
    "        selected_vector = vectors[selected_index].reshape(1, -1)\n",
    "\n",
    "        if method == 'cosine':\n",
    "            from scipy.spatial.distance import cdist\n",
    "            distances = cdist(remaining_vectors, selected_vector, metric='cosine').flatten()\n",
    "        elif method == 'euclidean':\n",
    "            distances = np.linalg.norm(remaining_vectors - selected_vector, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance method. Choose 'cosine' or 'euclidean'.\")\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def calculate_centroid(self, material_df, similarity_cols):\n",
    "        \"\"\"\n",
    "        Calculate the centroid based on the similarity columns.\n",
    "\n",
    "        Parameters:\n",
    "        - material_df (pd.DataFrame): The DataFrame containing material similarities.\n",
    "        - similarity_cols (list): The list of columns representing similarity scores.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: The centroid of the similarity scores, or None if any values are missing.\n",
    "        \"\"\"\n",
    "        if material_df[similarity_cols].isnull().any().any():\n",
    "            return None\n",
    "        similarity_data = material_df[similarity_cols].values\n",
    "        return similarity_data.mean(axis=0)\n",
    "\n",
    "    def run_workflow(self, abstracts_csv, materials_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Main workflow function for processing multiple material files.\n",
    "\n",
    "        Parameters:\n",
    "        - abstracts_csv: Path to the abstracts CSV file.\n",
    "        - materials_dir: Path to the directory containing material CSV files.\n",
    "        - output_dir: Path to the output directory for processed files and models.\n",
    "        \"\"\"\n",
    "        # Ensure the main output directory and subdirectories exist\n",
    "        output_dir = Path(output_dir)\n",
    "        processed_data_dir = output_dir / \"processed_data\"\n",
    "        output_models_dir = output_dir / \"output_models\"\n",
    "        selected_papers_dir = output_dir / \"selected_papers\"\n",
    "        centroid_history_dir = output_dir / \"centroid_history\"\n",
    "        processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        selected_papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "        centroid_history_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load and process abstracts\n",
    "        abstracts_df = pd.read_csv(abstracts_csv)\n",
    "        self.train_doc2vec(abstracts_df)\n",
    "\n",
    "        # Iterate through material files in the directory\n",
    "        material_files = list(Path(materials_dir).glob(\"*.csv\"))\n",
    "        for material_file in material_files:\n",
    "            print(f\"Processing material file: {material_file.name}\")\n",
    "\n",
    "            # Initialize variables for greedy selection\n",
    "            prev_centroid = None\n",
    "            valid_centroid = False\n",
    "            step = 1\n",
    "            accumulated_selected_indices = set()\n",
    "            centroid_history = []\n",
    "\n",
    "            while True:\n",
    "                selected_indices = self.greedy_selection()\n",
    "                accumulated_selected_indices.update(selected_indices)\n",
    "\n",
    "                # Get the selected papers\n",
    "                selected_papers = abstracts_df.iloc[list(accumulated_selected_indices)]\n",
    "\n",
    "                # Save the selected papers for the current material file\n",
    "                selected_papers_name = f\"{material_file.stem}_selected_papers.csv\"\n",
    "                selected_papers_path = selected_papers_dir / selected_papers_name\n",
    "                selected_papers.to_csv(selected_papers_path, index=False)\n",
    "\n",
    "                # Train Word2Vec model for the selected papers\n",
    "                self.train_word2vec(selected_papers, None)\n",
    "\n",
    "                # Process the current material file\n",
    "                material_df = pd.read_csv(material_file)\n",
    "                processed_material_df = self.process_materials(material_df)\n",
    "\n",
    "                # Save the processed material file\n",
    "                processed_file_name = f\"{material_file.stem}_with_similarity.csv\"\n",
    "                processed_file_path = processed_data_dir / processed_file_name\n",
    "                processed_material_df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "                # Calculate the centroid and check stopping condition\n",
    "                similarity_cols = self.config['materials_processing']['similarity_cols']\n",
    "                centroid = self.calculate_centroid(processed_material_df, similarity_cols)\n",
    "\n",
    "                if centroid is None:\n",
    "                    step += 1\n",
    "                    continue\n",
    "\n",
    "                if valid_centroid and prev_centroid is not None:\n",
    "                    distance = np.linalg.norm(centroid - prev_centroid)\n",
    "                    print(f\"Distance between centroids (step {step - 1} and {step}): {distance}\")\n",
    "\n",
    "                    # Append centroid and distance to history\n",
    "                    centroid_history.append({\n",
    "                        \"step\": step,\n",
    "                        \"centroid\": centroid.tolist(),\n",
    "                        \"distance_from_previous\": distance\n",
    "                    })\n",
    "\n",
    "                    if distance < self.config['threshold']:\n",
    "                        # Save final results specific to this material file\n",
    "                        final_model_path = output_models_dir / f\"{material_file.stem}_final.model\"\n",
    "                        self.train_word2vec(selected_papers, final_model_path)\n",
    "                        break\n",
    "                else:\n",
    "                    # Append the initial centroid (no distance yet)\n",
    "                    centroid_history.append({\n",
    "                        \"step\": step,\n",
    "                        \"centroid\": centroid.tolist(),\n",
    "                        \"distance_from_previous\": None\n",
    "                    })\n",
    "\n",
    "                prev_centroid = centroid\n",
    "                valid_centroid = True\n",
    "                step += 1\n",
    "\n",
    "            # Save centroid history for this material file\n",
    "            centroid_history_file = centroid_history_dir / f\"{material_file.stem}_centroid_history.csv\"\n",
    "            pd.DataFrame(centroid_history).to_csv(centroid_history_file, index=False)\n",
    "\n",
    "        print(\"Workflow completed for all material files.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"random_seed\": 42,\n",
    "    \"doc2vec\": {\n",
    "        \"vector_size\": 200,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    },\n",
    "    \"greedy_selection\": {\n",
    "        \"method\": \"cosine\",\n",
    "        \"use_pca\": True,\n",
    "        \"n_components\": 2,\n",
    "        \"start_size\": 50,\n",
    "        \"step_size\": 50\n",
    "    },\n",
    "    \"word2vec\": {\n",
    "        \"sg\": 1,\n",
    "        \"vector_size\": 200,\n",
    "        \"hs\": 1,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    },\n",
    "    \"materials_processing\": {\n",
    "        \"property_list\": [\"dielectric\", \"conductivity\"],\n",
    "        \"similarity_cols\": [\"Similarity_to_dielectric\", \"Similarity_to_conductivity\"]\n",
    "    },\n",
    "    \"threshold\": 0.04\n",
    "}"
   ],
   "id": "2d8829fb3cc10707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "pipeline = WorkflowPipeline(config)\n",
    "pipeline.run_workflow(\n",
    "    abstracts_csv=\"../01_collect_papers/clean_files/papers_until_2023.csv\",\n",
    "    materials_dir=\"../material_systems/MinDMaxC\",\n",
    "    output_dir=\"selection_results/MinDMaxC\"\n",
    ")"
   ],
   "id": "f825380bfe07d537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline.run_workflow(\n",
    "    abstracts_csv=\"../01_collect_papers/clean_files/papers_until_2023.csv\",\n",
    "    materials_dir=\"../material_systems/MaxDMinC\",\n",
    "    output_dir=\"selection_results/MaxDMinC\"\n",
    ")"
   ],
   "id": "1cbb367ce7c8b318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "59e600b7db9d08fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:matnexus_test_cross_platform]",
   "language": "python",
   "name": "conda-env-matnexus_test_cross_platform-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
