{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:46:10.748673Z",
     "start_time": "2025-02-19T20:46:10.716030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "from matnexus import VecGenerator\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class WorkflowPipeline:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with a configuration dictionary.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.random_seed = config.get('random_seed', 42)\n",
    "        self.PERIODIC_TABLE_ELEMENTS = [\n",
    "            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P',\n",
    "            'S', 'Cl', 'Ar', 'K', 'Ca',\n",
    "            'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se',\n",
    "            'Br', 'Kr', 'Rb', 'Sr', 'Y',\n",
    "            'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I',\n",
    "            'Xe', 'Cs', 'Ba', 'La', 'Ce',\n",
    "            'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf',\n",
    "            'Ta', 'W', 'Re', 'Os', 'Ir',\n",
    "            'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa',\n",
    "            'U', 'Np', 'Pu', 'Am', 'Cm',\n",
    "            'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds',\n",
    "            'Rg', 'Cn', 'Nh', 'Fl', 'Mc',\n",
    "            'Lv', 'Ts', 'Og'\n",
    "        ]\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "    def train_doc2vec(self, abstracts_df):\n",
    "        params = self.config['doc2vec']\n",
    "\n",
    "        # Filter and create a new copy to avoid SettingWithCopyWarning\n",
    "        abstracts_df = abstracts_df[abstracts_df['abstract'].notna()].copy()\n",
    "        abstracts_df['abstract'] = abstracts_df['abstract'].astype(str)\n",
    "\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=abstract.split(), tags=[str(i)])\n",
    "            for i, abstract in enumerate(abstracts_df['abstract'])\n",
    "        ]\n",
    "        model = Doc2Vec(\n",
    "            tagged_docs,\n",
    "            vector_size=params.get('vector_size', 50),\n",
    "            window=params.get('window', 5),\n",
    "            min_count=params.get('min_count', 1),\n",
    "            workers=params.get('workers', 4),\n",
    "            seed=self.random_seed\n",
    "        )\n",
    "        self.doc2vec_model = model\n",
    "        self.paper_vectors = np.array([model.dv[str(i)] for i in range(len(tagged_docs))])\n",
    "        return model\n",
    "\n",
    "    def train_word2vec(self, abstracts_df, output_path=None):\n",
    "        params = self.config['word2vec']\n",
    "        corpus = VecGenerator.Corpus(abstracts_df)\n",
    "        sentences = corpus.sentences\n",
    "        model = VecGenerator.Word2VecModel(sentences)\n",
    "        model.fit(\n",
    "            sg=params.get('sg', 1),\n",
    "            vector_size=params.get('vector_size', 100),\n",
    "            hs=params.get('hs', 1),\n",
    "            window=params.get('window', 5),\n",
    "            min_count=params.get('min_count', 1),\n",
    "            workers=params.get('workers', 4)\n",
    "        )\n",
    "        if output_path:  # Save only if an output path is specified\n",
    "            model.save(str(output_path))  # Convert Path to string\n",
    "        self.word2vec_model = model  # Keep the model in memory\n",
    "        return model\n",
    "\n",
    "    def process_materials(self, material_df):\n",
    "        params = self.config['materials_processing']\n",
    "        property_list = params['property_list']\n",
    "        elements = [col for col in material_df.columns if col in self.PERIODIC_TABLE_ELEMENTS]\n",
    "\n",
    "        if not hasattr(self, 'word2vec_model'):\n",
    "            raise ValueError(\"Word2Vec model not found. Ensure the model is trained before processing materials.\")\n",
    "\n",
    "        self.calculator = VecGenerator.MaterialSimilarityCalculator(self.word2vec_model)\n",
    "\n",
    "        for prop in property_list:\n",
    "            similarity_col = f\"Similarity_to_{prop}\"\n",
    "            try:\n",
    "                temp_df = self.calculator.calculate_similarity_from_dataframe(\n",
    "                    material_df,\n",
    "                    elements,\n",
    "                    target_property=[prop],\n",
    "                    add_experimental_indicator=False\n",
    "                )\n",
    "                material_df[similarity_col] = temp_df['Similarity']\n",
    "\n",
    "                if 'Material_Vec' in temp_df.columns:\n",
    "                    material_df['Material_Vec'] = temp_df['Material_Vec']\n",
    "                else:\n",
    "                    material_df['Material_Vec'] = np.nan\n",
    "            except Exception as e:\n",
    "                material_df[similarity_col] = np.nan\n",
    "\n",
    "        return material_df\n",
    "    def greedy_selection(self):\n",
    "        \"\"\"\n",
    "        Optimized greedy selection:\n",
    "        - Uses vectorized operations for distance calculations.\n",
    "        - Tracks minimum distances for unselected vectors to reduce redundant computations.\n",
    "        \"\"\"\n",
    "        params = self.config['greedy_selection']\n",
    "        vectors = self.paper_vectors\n",
    "\n",
    "        if params.get('use_pca', False):\n",
    "            n_components = params.get('n_components', 2)\n",
    "            pca = PCA(n_components=n_components)\n",
    "            vectors = pca.fit_transform(vectors)\n",
    "\n",
    "        # Parameters\n",
    "        start_size = params.get('start_size', 50)\n",
    "        step_size = params.get('step_size', 50)\n",
    "        method = params.get('method', 'cosine')\n",
    "\n",
    "        # Initialize selection for the first step\n",
    "        if not hasattr(self, 'selected_indices'):\n",
    "            # Compute the center of the vector space\n",
    "            vector_center = vectors.mean(axis=0)\n",
    "\n",
    "            # Calculate distances to the center\n",
    "            if method == 'cosine':\n",
    "                from scipy.spatial.distance import cdist\n",
    "                distances_to_center = cdist(vectors, vector_center.reshape(1, -1), metric='cosine').flatten()\n",
    "            elif method == 'euclidean':\n",
    "                distances_to_center = np.linalg.norm(vectors - vector_center, axis=1)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported distance method. Choose 'cosine' or 'euclidean'.\")\n",
    "\n",
    "            # Find the closest vector to the center\n",
    "            first_index = np.argmin(distances_to_center)\n",
    "\n",
    "            # Initialize the selected and remaining indices\n",
    "            self.selected_indices = [first_index]\n",
    "            self.remaining_indices = list(range(len(vectors)))\n",
    "            self.remaining_indices.remove(first_index)\n",
    "\n",
    "            # Calculate initial distances from all remaining vectors to the first selected vector\n",
    "            distances = self._compute_distances(vectors, first_index, method)\n",
    "            self.min_distances = distances.copy()  # Track minimum distances for efficiency\n",
    "\n",
    "            # Select the rest of the first batch\n",
    "            while len(self.selected_indices) < start_size:\n",
    "                self._select_next_furthest(vectors, method)\n",
    "\n",
    "            # Set the target size for subsequent steps\n",
    "            self.target_size = start_size\n",
    "\n",
    "        else:\n",
    "            # Increment target size for this batch\n",
    "            self.target_size += step_size\n",
    "\n",
    "            # Add only the new `step_size` papers\n",
    "            while len(self.selected_indices) < self.target_size:\n",
    "                self._select_next_furthest(vectors, method)\n",
    "\n",
    "        return self.selected_indices\n",
    "\n",
    "    def _select_next_furthest(self, vectors, method):\n",
    "        \"\"\"\n",
    "        Helper method to select the next furthest vector using tracked minimum distances.\n",
    "        \"\"\"\n",
    "        # Identify the furthest vector from the selected set\n",
    "        furthest_index = np.argmax(self.min_distances)\n",
    "        actual_index = self.remaining_indices[furthest_index]\n",
    "\n",
    "        # Append the furthest vector to the selected set\n",
    "        self.selected_indices.append(actual_index)\n",
    "\n",
    "        # Remove the selected vector from the remaining set\n",
    "        del self.remaining_indices[furthest_index]\n",
    "\n",
    "        # Remove the corresponding entry in min_distances\n",
    "        self.min_distances = np.delete(self.min_distances, furthest_index)\n",
    "\n",
    "        # Compute new distances for the remaining vectors\n",
    "        new_distances = self._compute_distances(vectors, actual_index, method)\n",
    "\n",
    "        # Update the minimum distances\n",
    "        self.min_distances = np.minimum(self.min_distances, new_distances)\n",
    "\n",
    "    def _compute_distances(self, vectors, selected_index, method):\n",
    "        \"\"\"\n",
    "        Compute distances from one vector to all remaining vectors using the specified method.\n",
    "        \"\"\"\n",
    "        remaining_vectors = vectors[self.remaining_indices]\n",
    "        selected_vector = vectors[selected_index].reshape(1, -1)\n",
    "\n",
    "        if method == 'cosine':\n",
    "            from scipy.spatial.distance import cdist\n",
    "            distances = cdist(remaining_vectors, selected_vector, metric='cosine').flatten()\n",
    "        elif method == 'euclidean':\n",
    "            distances = np.linalg.norm(remaining_vectors - selected_vector, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance method. Choose 'cosine' or 'euclidean'.\")\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def calculate_centroid(self, material_df, similarity_cols):\n",
    "        \"\"\"\n",
    "        Calculate the centroid based on the similarity columns.\n",
    "\n",
    "        Parameters:\n",
    "        - material_df (pd.DataFrame): The DataFrame containing material similarities.\n",
    "        - similarity_cols (list): The list of columns representing similarity scores.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: The centroid of the similarity scores, or None if any values are missing.\n",
    "        \"\"\"\n",
    "        if material_df[similarity_cols].isnull().any().any():\n",
    "            return None\n",
    "        similarity_data = material_df[similarity_cols].values\n",
    "        return similarity_data.mean(axis=0)\n",
    "\n",
    "    def _reset_selection_state(self):\n",
    "        if hasattr(self, 'selected_indices'):\n",
    "            del self.selected_indices\n",
    "        if hasattr(self, 'remaining_indices'):\n",
    "            del self.remaining_indices\n",
    "        if hasattr(self, 'min_distances'):\n",
    "            del self.min_distances\n",
    "        if hasattr(self, 'target_size'):\n",
    "            del self.target_size\n",
    "\n",
    "    def run_workflow(self, abstracts_csv, materials_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Main workflow function for processing multiple material files.\n",
    "\n",
    "        Parameters:\n",
    "        - abstracts_csv: Path to the abstracts CSV file.\n",
    "        - materials_dir: Path to the directory containing material CSV files.\n",
    "        - output_dir: Path to the output directory for processed files and models.\n",
    "        \"\"\"\n",
    "        self._reset_selection_state()\n",
    "\n",
    "        # Ensure the main output directory and subdirectories exist\n",
    "        output_dir = Path(output_dir)\n",
    "        processed_data_dir = output_dir / \"processed_data\"\n",
    "        output_models_dir = output_dir / \"output_models\"\n",
    "        selected_papers_dir = output_dir / \"selected_papers\"\n",
    "        centroid_history_dir = output_dir / \"centroid_history\"\n",
    "        processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        selected_papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "        centroid_history_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load and process abstracts\n",
    "        abstracts_df = pd.read_csv(abstracts_csv)\n",
    "        self.train_doc2vec(abstracts_df)\n",
    "\n",
    "        # Iterate through material files in the directory\n",
    "        material_files = list(Path(materials_dir).glob(\"*.csv\"))\n",
    "        for material_file in material_files:\n",
    "            self._reset_selection_state()\n",
    "            print(f\"Processing material file: {material_file.name}\")\n",
    "\n",
    "            # Initialize variables for greedy selection\n",
    "            prev_centroid = None\n",
    "            valid_centroid = False\n",
    "            step = 1\n",
    "            accumulated_selected_indices = set()\n",
    "            centroid_history = []\n",
    "\n",
    "\n",
    "            while True:\n",
    "                selected_indices = self.greedy_selection()\n",
    "                accumulated_selected_indices.update(selected_indices)\n",
    "\n",
    "                # Get the selected papers\n",
    "                selected_papers = abstracts_df.iloc[list(accumulated_selected_indices)]\n",
    "\n",
    "                # Save the selected papers for the current material file\n",
    "                selected_papers_name = f\"{material_file.stem}_selected_papers.csv\"\n",
    "                selected_papers_path = selected_papers_dir / selected_papers_name\n",
    "                selected_papers.to_csv(selected_papers_path, index=False)\n",
    "\n",
    "                # Train Word2Vec model for the selected papers\n",
    "                self.train_word2vec(selected_papers, None)\n",
    "\n",
    "                # Process the current material file\n",
    "                material_df = pd.read_csv(material_file)\n",
    "                processed_material_df = self.process_materials(material_df)\n",
    "\n",
    "                # Save the processed material file\n",
    "                processed_file_name = f\"{material_file.stem}_with_similarity.csv\"\n",
    "                processed_file_path = processed_data_dir / processed_file_name\n",
    "                processed_material_df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "                # Calculate the centroid and check stopping condition\n",
    "                similarity_cols = self.config['materials_processing']['similarity_cols']\n",
    "                centroid = self.calculate_centroid(processed_material_df, similarity_cols)\n",
    "\n",
    "                if centroid is None:\n",
    "                    step += 1\n",
    "                    continue\n",
    "\n",
    "                if valid_centroid and prev_centroid is not None:\n",
    "                    distance = np.linalg.norm(centroid - prev_centroid)\n",
    "                    print(f\"Distance between centroids (step {step - 1} and {step}): {distance}\")\n",
    "\n",
    "                    # Append centroid and distance to history\n",
    "                    centroid_history.append({\n",
    "                        \"step\": step,\n",
    "                        \"centroid\": centroid.tolist(),\n",
    "                        \"distance_from_previous\": distance\n",
    "                    })\n",
    "\n",
    "                    if distance < self.config['threshold']:\n",
    "                        # Save final results specific to this material file\n",
    "                        final_model_path = output_models_dir / f\"{material_file.stem}_final.model\"\n",
    "                        self.train_word2vec(selected_papers, final_model_path)\n",
    "                        break\n",
    "                else:\n",
    "                    # Append the initial centroid (no distance yet)\n",
    "                    centroid_history.append({\n",
    "                        \"step\": step,\n",
    "                        \"centroid\": centroid.tolist(),\n",
    "                        \"distance_from_previous\": None\n",
    "                    })\n",
    "\n",
    "                prev_centroid = centroid\n",
    "                valid_centroid = True\n",
    "                step += 1\n",
    "\n",
    "            # Save centroid history for this material file\n",
    "            centroid_history_file = centroid_history_dir / f\"{material_file.stem}_centroid_history.csv\"\n",
    "            pd.DataFrame(centroid_history).to_csv(centroid_history_file, index=False)\n",
    "\n",
    "        print(\"Workflow completed for all material files.\")"
   ],
   "id": "63f2a54494942eb1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:46:11.338961Z",
     "start_time": "2025-02-19T20:46:11.334740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"random_seed\": 42,\n",
    "    \"doc2vec\": {\n",
    "        \"vector_size\": 200,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    },\n",
    "    \"greedy_selection\": {\n",
    "        \"method\": \"cosine\",\n",
    "        \"use_pca\": True,\n",
    "        \"n_components\": 2,\n",
    "        \"start_size\": 50,\n",
    "        \"step_size\": 50\n",
    "    },\n",
    "    \"word2vec\": {\n",
    "        \"sg\": 1,\n",
    "        \"vector_size\": 200,\n",
    "        \"hs\": 1,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    },\n",
    "    \"materials_processing\": {\n",
    "        \"property_list\": [\"dielectric\", \"conductivity\"],\n",
    "        \"similarity_cols\": [\"Similarity_to_dielectric\", \"Similarity_to_conductivity\"]\n",
    "    },\n",
    "    \"threshold\": 0.03\n",
    "}"
   ],
   "id": "2d8829fb3cc10707",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T21:06:11.324492Z",
     "start_time": "2025-02-19T20:59:30.407211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pipeline = WorkflowPipeline(config)\n",
    "pipeline.run_workflow(\n",
    "    abstracts_csv=\"../01_collect_papers/clean_files/papers_until_2023.csv\",\n",
    "    materials_dir=\"../material_systems/MinDMaxC\",\n",
    "    output_dir=\"selection_results/MinDMaxC\"\n",
    ")"
   ],
   "id": "f825380bfe07d537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing material file: Ag_Pd_Pt_Ru_material_system.csv\n",
      "Distance between centroids (step 10 and 11): 0.04185003174440057\n",
      "Distance between centroids (step 11 and 12): 0.1466024318450898\n",
      "Distance between centroids (step 12 and 13): 0.04883597095702356\n",
      "Distance between centroids (step 13 and 14): 0.056580480496596953\n",
      "Distance between centroids (step 14 and 15): 0.03844229034628751\n",
      "Distance between centroids (step 15 and 16): 0.028131350532153522\n",
      "Processing material file: Ag_Pd_Ru_material_system.csv\n",
      "Distance between centroids (step 8 and 9): 0.10396473520149048\n",
      "Distance between centroids (step 9 and 10): 0.08724716216717188\n",
      "Distance between centroids (step 10 and 11): 0.09242270285200672\n",
      "Distance between centroids (step 11 and 12): 0.06816509895326692\n",
      "Distance between centroids (step 12 and 13): 0.08913855942699961\n",
      "Distance between centroids (step 13 and 14): 0.10378815716539247\n",
      "Distance between centroids (step 14 and 15): 0.13979896329372735\n",
      "Distance between centroids (step 15 and 16): 0.062187885851895094\n",
      "Distance between centroids (step 16 and 17): 0.1529982108261326\n",
      "Distance between centroids (step 17 and 18): 0.02990832057964804\n",
      "Processing material file: Ag_Au_Pd_Pt_Ru_material_system.csv\n",
      "Distance between centroids (step 12 and 13): 0.05142648120012076\n",
      "Distance between centroids (step 13 and 14): 0.04118629163151885\n",
      "Distance between centroids (step 14 and 15): 0.1582078495011982\n",
      "Distance between centroids (step 15 and 16): 0.050435549635411046\n",
      "Distance between centroids (step 16 and 17): 0.05262795033180658\n",
      "Distance between centroids (step 17 and 18): 0.050855999196476015\n",
      "Distance between centroids (step 18 and 19): 0.09012220783681997\n",
      "Distance between centroids (step 19 and 20): 0.09763515842405826\n",
      "Distance between centroids (step 20 and 21): 0.10193833911910598\n",
      "Distance between centroids (step 21 and 22): 0.03189788655851391\n",
      "Distance between centroids (step 22 and 23): 0.07639812688545253\n",
      "Distance between centroids (step 23 and 24): 0.01425076668982661\n",
      "Processing material file: Ag_Pd_Pt_material_system.csv\n",
      "Distance between centroids (step 3 and 4): 0.07491048282143271\n",
      "Distance between centroids (step 4 and 5): 0.034751546616708935\n",
      "Distance between centroids (step 5 and 6): 0.040846223816091425\n",
      "Distance between centroids (step 6 and 7): 0.13603345887772836\n",
      "Distance between centroids (step 7 and 8): 0.017832314391462482\n",
      "Processing material file: Ag_Au_Pd_Pt_Rh_material_system.csv\n",
      "Distance between centroids (step 8 and 9): 0.05216940861921943\n",
      "Distance between centroids (step 9 and 10): 0.04839180943863245\n",
      "Distance between centroids (step 10 and 11): 0.10870261575882101\n",
      "Distance between centroids (step 11 and 12): 0.052570889698587225\n",
      "Distance between centroids (step 12 and 13): 0.03287452202060928\n",
      "Distance between centroids (step 13 and 14): 0.055625173520907005\n",
      "Distance between centroids (step 14 and 15): 0.025835072974084116\n",
      "Workflow completed for all material files.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T21:08:05.596993Z",
     "start_time": "2025-02-19T21:06:23.428821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = WorkflowPipeline(config)\n",
    "pipeline.run_workflow(\n",
    "    abstracts_csv=\"../01_collect_papers/clean_files/papers_until_2023.csv\",\n",
    "    materials_dir=\"../material_systems/MaxDMinC\",\n",
    "    output_dir=\"selection_results/MaxDMinC\"\n",
    ")"
   ],
   "id": "1cbb367ce7c8b318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing material file: Ni_Pd_Pt_Ru_material_system.csv\n",
      "Distance between centroids (step 10 and 11): 0.06561696445475504\n",
      "Distance between centroids (step 11 and 12): 0.11812458498467135\n",
      "Distance between centroids (step 12 and 13): 0.0470311121063927\n",
      "Distance between centroids (step 13 and 14): 0.02940765951725515\n",
      "Workflow completed for all material files.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "59e600b7db9d08fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env1]",
   "language": "python",
   "name": "conda-env-env1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
